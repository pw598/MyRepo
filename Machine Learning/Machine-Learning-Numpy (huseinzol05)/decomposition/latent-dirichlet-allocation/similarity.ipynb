{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import re, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kerajaan','r') as fopen:\n",
    "    kerajaan = list(filter(None, fopen.read().split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = ' '.join(string)\n",
    "    return string.lower()\n",
    "\n",
    "kerajaan = [clearstring(i) for i in kerajaan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_index(p):\n",
    "    return np.random.multinomial(1,p).argmax()\n",
    "\n",
    "def word_indices(vec):\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in range(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K=None):\n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K*alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self, corpus, n_topics, iteration=30, alpha=0.1, beta=0.1):\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = list(set(' '.join(self.corpus).split()))\n",
    "        self.iteration = iteration\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_topics = n_topics\n",
    "        self._bow()\n",
    "        n_docs, vocab_size = self.tfidf.shape\n",
    "        self.nmz = np.zeros((n_docs, n_topics))\n",
    "        self.nzw = np.zeros((n_topics, vocab_size))\n",
    "        self.nm = np.zeros(n_docs)\n",
    "        self.nz = np.zeros(self.n_topics)\n",
    "        self.topics = {}\n",
    "        \n",
    "        for m in range(n_docs):\n",
    "            for i, w in enumerate(word_indices(self.tfidf[m, :])):\n",
    "                z = np.random.randint(n_topics)\n",
    "                self.nmz[m,z] += 1\n",
    "                self.nm[m] += 1\n",
    "                self.nzw[z,w] += 1\n",
    "                self.nz[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "        \n",
    "    def _bow(self):\n",
    "        self.tfidf = np.zeros((len(self.corpus),len(self.vocabulary)))\n",
    "        for no, i in enumerate(self.corpus):\n",
    "            for text in i.split():\n",
    "                self.tfidf[no, self.vocabulary.index(text)] += 1\n",
    "                \n",
    "    def _conditional_distribution(self, m, w):\n",
    "        vocab_size = self.nzw.shape[1]\n",
    "        left = (self.nzw[:,w] + self.beta) / (self.nz + self.beta * vocab_size)\n",
    "        right = (self.nmz[m,:] + self.alpha) / (self.nm[m] + self.alpha * self.n_topics)\n",
    "        p_z = left * right\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "                \n",
    "    def loglikelihood(self):\n",
    "        vocab_size = self.nzw.shape[1]\n",
    "        n_docs = self.nmz.shape[0]\n",
    "        lik = 0\n",
    "        for z in range(self.n_topics):\n",
    "            lik += log_multi_beta(self.nzw[z,:]+self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vocab_size)\n",
    "        for m in range(n_docs):\n",
    "            lik += log_multi_beta(self.nmz[m,:]+self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.n_topics)\n",
    "        return lik\n",
    "    \n",
    "    def run(self):\n",
    "        for it in range(self.iteration):\n",
    "            for m in range(self.tfidf.shape[0]):\n",
    "                for i, w in enumerate(word_indices(self.tfidf[m, :])):\n",
    "                    z = self.topics[(m,i)]\n",
    "                    self.nmz[m,z] -= 1\n",
    "                    self.nm[m] -= 1\n",
    "                    self.nzw[z,w] -= 1\n",
    "                    self.nz[z] -= 1\n",
    "                    p_z = self._conditional_distribution(m, w)\n",
    "                    z = sample_index(p_z)\n",
    "                    self.nmz[m,z] += 1\n",
    "                    self.nm[m] += 1\n",
    "                    self.nzw[z,w] += 1\n",
    "                    self.nz[z] += 1\n",
    "                    self.topics[(m,i)] = z\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences(keyword, corpus):\n",
    "    d = []\n",
    "    for content in [i for i in corpus if i.find(keyword)>=0]:\n",
    "        a = content.split()\n",
    "        d.append(a)\n",
    "    return ' '.join([j for i in d for j in i if re.match(\"^[a-zA-Z_-]*$\", j) and len(j) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(string1, string2, corpus, use_tfidf=True, epoch=50, learning_rate=1e-6, lam=1e3, penalty=1e-6):\n",
    "    queries = [find_sentences(string1, corpus), find_sentences(string2, corpus)]\n",
    "    lda = LDA(queries,2)\n",
    "    lda.run()\n",
    "    a=lda.nmz.dot(lda.nzw)\n",
    "    angles=np.arccos(np.dot(a[0,:],a[1:].T) / (np.linalg.norm(a[0,:],2)* np.linalg.norm(a[1:],2)))\n",
    "    return np.abs(1 - float(angles[0])/float(np.pi/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991450069748095"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('kedah', 'kedah', kerajaan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34125433347880385"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('kedah', 'dap', kerajaan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3489930111865034"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('kedah', 'bn', kerajaan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
