{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kerajaan','r') as fopen:\n",
    "    kerajaan = list(filter(None, fopen.read().split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = ' '.join(string)\n",
    "    return string.lower()\n",
    "\n",
    "kerajaan = [clearstring(i) for i in kerajaan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(M, mu):\n",
    "    return np.where(M>=mu,0, np.min(M - mu, 0))\n",
    "\n",
    "def grads(M, W, H, lam, mu):\n",
    "    R = W.dot(H) - M\n",
    "    return R.dot(H.T) + penalty(W, mu)*lam, W.T.dot(R) + penalty(H, mu)*lam\n",
    "\n",
    "def upd(M, W, H, lr, lam, mu):\n",
    "    dW,dH = grads(M,W,H,lam,mu)\n",
    "    W -= lr*dW\n",
    "    H -= lr*dH\n",
    "    \n",
    "def tfidf(corpus):\n",
    "    vocabulary = list(set(' '.join(corpus).split()))\n",
    "    idf = {}\n",
    "    for i in vocabulary:\n",
    "        idf[i] = 0\n",
    "        for k in corpus:\n",
    "            if i in k.split():\n",
    "                idf[i] += 1\n",
    "        idf[i] = np.log(idf[i] / len(corpus))\n",
    "    tfidf = np.zeros((len(corpus),len(vocabulary)))\n",
    "    for no, i in enumerate(corpus):\n",
    "        for text in i.split():\n",
    "            tfidf[no, vocabulary.index(text)] += 1\n",
    "        for text in i.split():\n",
    "            tfidf[no, vocabulary.index(text)] = tfidf[no, vocabulary.index(text)] * idf[text]\n",
    "    return vocabulary, tfidf\n",
    "\n",
    "def bow(corpus):\n",
    "    vocabulary = list(set(' '.join(corpus).split()))\n",
    "    bow = np.zeros((len(corpus),len(vocabulary)))\n",
    "    for no, i in enumerate(corpus):\n",
    "        for text in i.split():\n",
    "            bow[no, vocabulary.index(text)] += 1\n",
    "    return vocabulary, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences(keyword, corpus):\n",
    "    d = []\n",
    "    for content in [i for i in corpus if i.find(keyword)>=0]:\n",
    "        a = content.split()\n",
    "        d.append(a)\n",
    "    return ' '.join([j for i in d for j in i if re.match(\"^[a-zA-Z_-]*$\", j) and len(j) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(string1, string2, corpus, use_tfidf=True, epoch=50, learning_rate=1e-6, lam=1e3, penalty=1e-6):\n",
    "    queries = [find_sentences(string1, corpus), find_sentences(string2, corpus)]\n",
    "    if use_tfidf:\n",
    "        vocab, vectors = tfidf(queries)\n",
    "    else:\n",
    "        vocab, vectors = bow(queries)\n",
    "    m, n = vectors.shape\n",
    "    W = np.abs(np.random.normal(scale=0.01, size=(m,2)))\n",
    "    H = np.abs(np.random.normal(scale=0.01, size=(2,n)))\n",
    "    for i in range(epoch):\n",
    "        upd(vectors,W,H,learning_rate,lam,penalty)\n",
    "    a=W.dot(H)\n",
    "    angles=np.arccos(np.dot(a[0,:],a[1:].T) / (np.linalg.norm(a[0,:],2)* np.linalg.norm(a[1:],2)))\n",
    "    return np.abs(1 - float(angles[0])/float(np.pi/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896504454896407"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('kedah', 'kedah', kerajaan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923361608472873"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('kedah', 'dap', kerajaan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7958667334387592"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('kedah', 'bn', kerajaan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
