{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = ' '.join(string)\n",
    "    return string\n",
    "\n",
    "def separate_dataset(trainset):\n",
    "    datastring = []\n",
    "    datatarget = []\n",
    "    for i in range(len(trainset.data)):\n",
    "        data_ = trainset.data[i].split('\\n')\n",
    "        data_ = list(filter(None, data_))\n",
    "        for n in range(len(data_)):\n",
    "            data_[n] = clearstring(data_[n])\n",
    "        datastring += data_\n",
    "        for n in range(len(data_)):\n",
    "            datatarget.append(trainset.target[i])\n",
    "    return datastring, datatarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kerajaan', 'pembangkang']\n",
      "201\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "trainset = sklearn.datasets.load_files(container_path = 'local', encoding = 'UTF-8')\n",
    "trainset.data, trainset.target = separate_dataset(trainset)\n",
    "print (trainset.target_names)\n",
    "print (len(trainset.data))\n",
    "print (len(trainset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1737"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(set(' '.join(trainset.data).split()))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.30330491, 5.30330491, 5.30330491, ..., 5.30330491, 4.61015773,\n",
       "       5.30330491])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.zeros((len(vocabulary)))\n",
    "for no, i in enumerate(vocabulary):\n",
    "    for s in trainset.data:\n",
    "        if i in s.split():\n",
    "            idf[no] += 1\n",
    "    idf[no] = np.log(len(trainset.data) / idf[no])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.zeros((len(trainset.data),len(vocabulary)))\n",
    "for no, s in enumerate(trainset.data):\n",
    "    for w in s.split():\n",
    "        tf[no, vocabulary.index(w)] += 1\n",
    "tfidf = tf * idf\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(trainset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 8359.847501116172, 1: 5170.803704701069}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_total_tfidf = {}\n",
    "for l in np.unique(label):\n",
    "    store_total_tfidf[l] = tfidf[np.where(label == l)[0]].sum()\n",
    "store_total_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49751244, 0.50248756])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = np.zeros([len(trainset.target_names)])\n",
    "counts = np.unique(label, return_counts = True)\n",
    "for i in range(len(counts[0])):\n",
    "    prob[counts[0][i]] = counts[1][i] / label.shape[0]\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tfidf = np.zeros((len(store_total_tfidf), tfidf.shape[1]))\n",
    "trained_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.37890444, -7.37890444, -7.37890444, ..., -7.37890444,\n",
       "        -6.89560107, -9.21997853],\n",
       "       [-8.84040702, -8.84040702, -8.84040702, ..., -8.84040702,\n",
       "        -8.84040702, -6.99933294]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tfidf[0] = np.log((tfidf[np.where(label == 0)[0]].sum(axis=0) + 1) / (store_total_tfidf[0] + tfidf.shape[1]))\n",
    "trained_tfidf[1] = np.log((tfidf[np.where(label == 1)[0]].sum(axis=0) + 1) / (store_total_tfidf[1] + tfidf.shape[1]))\n",
    "trained_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    mx = np.max(x, axis=-1, keepdims=True)\n",
    "    top = np.exp(x - mx)\n",
    "    bottom = np.sum(top, axis = 1, keepdims = True)\n",
    "    return top / bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.13435465e-63, 1.00000000e+00],\n",
       "       [1.16575968e-45, 1.00000000e+00],\n",
       "       [1.23479225e-81, 1.00000000e+00],\n",
       "       [1.20714646e-38, 1.00000000e+00],\n",
       "       [1.19345584e-41, 1.00000000e+00],\n",
       "       [4.00001684e-70, 1.00000000e+00],\n",
       "       [5.06942386e-73, 1.00000000e+00],\n",
       "       [1.49247236e-43, 1.00000000e+00],\n",
       "       [3.29223786e-40, 1.00000000e+00],\n",
       "       [1.87686650e-32, 1.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_result = tfidf.dot(trained_tfidf.T) + np.log(prob)\n",
    "softmax(new_result)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(tfidf, trainset.target, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.label = np.array(Y)\n",
    "        self.store_total = {}\n",
    "        for l in np.unique(label):\n",
    "            self.store_total[l] = X[np.where(self.label == l)[0]].sum()\n",
    "        \n",
    "        self.prob = np.zeros([len(self.store_total)])\n",
    "        counts = np.unique(self.label, return_counts = True)\n",
    "        for i in range(len(counts[0])):\n",
    "            self.prob[counts[0][i]] = counts[1][i] / self.label.shape[0]\n",
    "        \n",
    "        self.log_prob = np.zeros((len(self.store_total), X.shape[1]))\n",
    "        \n",
    "        for no, l in enumerate(np.unique(label)):\n",
    "            top = X[np.where(self.label == 0)[0]].sum(axis=0) + 1\n",
    "            bottom = self.store_total[l] + X.shape[1]\n",
    "            self.log_prob[no] = np.log(top / bottom)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        new_result = X.dot(self.log_prob.T) + np.log(self.prob)\n",
    "        return softmax(new_result)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_bayes = MultinomialNB()\n",
    "multinomial_bayes.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_Y == multinomial_bayes.predict(train_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5365853658536586"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_Y == multinomial_bayes.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32087101e-09, 9.99999999e-01],\n",
       "       [1.18502362e-14, 1.00000000e+00],\n",
       "       [9.38632721e-20, 1.00000000e+00],\n",
       "       [4.55619638e-06, 9.99995444e-01],\n",
       "       [4.07506407e-18, 1.00000000e+00],\n",
       "       [3.33260593e-16, 1.00000000e+00],\n",
       "       [1.26882112e-17, 1.00000000e+00],\n",
       "       [1.94283519e-09, 9.99999998e-01],\n",
       "       [2.63618910e-17, 1.00000000e+00],\n",
       "       [4.21985803e-10, 1.00000000e+00],\n",
       "       [7.13460635e-08, 9.99999929e-01],\n",
       "       [3.76005742e-09, 9.99999996e-01],\n",
       "       [1.39409848e-07, 9.99999861e-01],\n",
       "       [5.47949491e-16, 1.00000000e+00],\n",
       "       [8.56484791e-17, 1.00000000e+00],\n",
       "       [3.59546649e-07, 9.99999640e-01],\n",
       "       [2.05689487e-15, 1.00000000e+00],\n",
       "       [1.52764013e-09, 9.99999998e-01],\n",
       "       [1.16722178e-15, 1.00000000e+00],\n",
       "       [4.29137996e-10, 1.00000000e+00],\n",
       "       [1.33689912e-10, 1.00000000e+00],\n",
       "       [5.58334180e-08, 9.99999944e-01],\n",
       "       [2.15378196e-16, 1.00000000e+00],\n",
       "       [2.37915138e-08, 9.99999976e-01],\n",
       "       [5.05813030e-03, 9.94941870e-01],\n",
       "       [3.76546343e-08, 9.99999962e-01],\n",
       "       [6.97936017e-11, 1.00000000e+00],\n",
       "       [4.18292036e-15, 1.00000000e+00],\n",
       "       [3.33696519e-10, 1.00000000e+00],\n",
       "       [1.26169725e-16, 1.00000000e+00],\n",
       "       [1.13428944e-14, 1.00000000e+00],\n",
       "       [8.91323138e-14, 1.00000000e+00],\n",
       "       [1.63680420e-15, 1.00000000e+00],\n",
       "       [1.18360990e-12, 1.00000000e+00],\n",
       "       [1.32920662e-14, 1.00000000e+00],\n",
       "       [3.26577412e-07, 9.99999673e-01],\n",
       "       [3.29165917e-13, 1.00000000e+00],\n",
       "       [2.32454831e-13, 1.00000000e+00],\n",
       "       [2.71827064e-14, 1.00000000e+00],\n",
       "       [4.12270946e-14, 1.00000000e+00],\n",
       "       [1.27854064e-08, 9.99999987e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_bayes.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
